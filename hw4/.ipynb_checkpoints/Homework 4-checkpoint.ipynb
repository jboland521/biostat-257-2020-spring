{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "### BIOSTAT 257\n",
    "### Joanna Boland\n",
    "### May 29, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we continue with the linear mixed effects model (LMM) considered in HW2\n",
    "\n",
    "$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,$$\n",
    "where\n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,\n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,\n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,\n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$, \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and\n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is\n",
    "$$\n",
    "\\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T,\n",
    "$$\n",
    "Because the variance component parameter $\\boldsymbol{\\Sigma}$ has to be positive semidefinite. We use its Cholesky factor $\\mathbf{L}$ as optimization variable.\n",
    "\n",
    "Given $m$ independent data points $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2).\n",
    "$$\n",
    "In this assignment, we use the nonlinear programming (NLP) approach for optimization. In HW5 and HW6, we will derive an EM (expectation-maximization) algorithm and an MM (minorization-maximization) algorithm for the same problem.\n",
    "\n",
    "### Question 1: Derivatives\n",
    "\n",
    "NLP optimization solvers expect users to provide at least a function for evaluating objective value. If users can provide further information such as gradient and Hessian, the NLP solvers will be more stable and converge faster. Automatic differentiation tools are becoming more powerful but cannot apply to all problems yet.\n",
    "\n",
    "#### 1. Gradients \n",
    "\n",
    "Letting $\\mathbf{r}_i = \\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}$ we can prove the gradients as such:\n",
    "\n",
    "First, we can calculate $\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)$\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{1}{2} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}}(\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})$$ \n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{1}{2} \\times - 2\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})$$ \n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i$$ \n",
    "\n",
    "Then, $\\nabla_{\\boldsymbol{\\sigma^2}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2)$, we separate into two components\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\boldsymbol{\\sigma^2}}\\text{logdet}(\\boldsymbol{\\Omega}_i) = \\text{Tr}\\Bigg( \\boldsymbol{\\Omega}_i^{-1}\\frac{\\partial}{\\partial \\boldsymbol{\\sigma^2}}\\boldsymbol{\\Omega}_i\\Bigg) = \\text{Tr}(\\boldsymbol{\\Omega}_i^{-1})$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\boldsymbol{\\sigma^2}}\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-1}\\mathbf{r}_i = \\frac{\\partial}{\\partial \\boldsymbol{\\sigma^2}}[-\\boldsymbol{\\sigma^2}\\mathbf{I}_{n_i}]\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i = -\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i$$\n",
    "\n",
    "$$\\therefore \\nabla_{\\boldsymbol{\\sigma^2}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = -\\frac{1}{2}\\text{Tr}(\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2}\\mathbf{r}_i^T\\boldsymbol{\\Omega}_i^{-2}\\mathbf{r}_i$$\n",
    "\n",
    "### Question 2: Objective and Gradient Evaluator for a Single Datum\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 8,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": 8,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions\n",
    "using Ipopt, LinearAlgebra, MathProgBase, MixedModels, NLopt\n",
    "using Random, RCall, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in HW2."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 231,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": 231,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 20,
=======
     "execution_count": 231,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
     "execution_count": 231,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    ∇β         :: Vector{T}\n",
    "    ∇σ²        :: Vector{T}\n",
    "    ∇Σ         :: Matrix{T}    \n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2 :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    C          :: Matrix{T}\n",
    "    Czty       :: Vector{T}\n",
    "    Cztxβ      :: Vector{T}\n",
    "    Cztz       :: Matrix{T}\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    ztzCzty    :: Vector{T}\n",
    "    ztzCztxβ   :: Vector{T}\n",
=======
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    ∇β         = Vector{T}(undef, p)\n",
    "    ∇σ²        = Vector{T}(undef, 1)\n",
    "    ∇Σ         = Matrix{T}(undef, q, q)    \n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2 = Vector{Float64}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    C          = Matrix{T}(undef, q, q)\n",
    "    Czty       = Vector{T}(undef, q)\n",
    "    ztxβ       = Vector{Float64}(undef, q)\n",
    "    Cztxβ      = Vector{T}(undef, q)\n",
    "    Cztz       = Matrix{T}(undef, q, q)\n",
    "    ztzCzty    = Vector{T}(undef, q)\n",
    "    ztzCztxβ   = Vector{T}(undef, q)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_n, storage_q2, \n",
    "        xtx, ztx, ztz, storage_qq, C, Czty, ztxβ, Cztxβ, \n",
    "        Cztz, ztzCzty, ztzCztxβ)\n",
=======
    "    C          = Matrix{Float64}(undef, q, q)\n",
    "    Czty       = Vector{Float64}(undef, q)\n",
    "    Cztxβ      = Vector{Float64}(undef, q)\n",
    "    Cztz       = Matrix{Float64}(undef, q, q)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2, \n",
    "        xtx, ztx, ztz, storage_qq, C, Czty, Cztxβ, Cztz)\n",
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
    "    C          = Matrix{Float64}(undef, q, q)\n",
    "    Czty       = Vector{Float64}(undef, q)\n",
    "    Cztxβ      = Vector{Float64}(undef, q)\n",
    "    Cztz       = Matrix{Float64}(undef, q, q)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇Σ, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2, \n",
    "        xtx, ztx, ztz, storage_qq, C, Czty, Cztxβ, Cztz)\n",
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, L, σ², needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `L`, \n",
    "and `σ²`. If `needgrad==true`, then `obs.∇β`, `obs.∇Σ`, and `obs.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs      :: LmmObs{T}, \n",
    "        β        :: Vector{T}, \n",
    "        L        :: Matrix{T}, \n",
    "        σ²       :: T,\n",
    "        needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    \n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    # form the q-by-q matrix: M = σ² * I + Lt Zt Z L\n",
    "    copy!(obs.storage_qq, obs.ztz) \n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq) # O(q^3) \n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq) # O(q^3) \n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ² \n",
    "    end\n",
    "    # cholesky on M = σ² * I + Lt Zt Z L\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res)) \n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.storage_q, obs.zty)) # O(pq)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q) # O(q^2) \n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, obs.storage_q) # O(q^3) \n",
    "    # l2 norm of residual vector\n",
    "    copy!(obs.storage_p, obs.xty)\n",
    "    rtr  = obs.yty +\n",
    "        dot(β, BLAS.gemv!('N', T(1), obs.xtx, β, T(-2), obs.storage_p))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (rtr - qf) / σ² \n",
    "    logl /= -2\n",
    "    \n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################    \n",
    "    if needgrad \n",
    "        ## C = LR^-1R^-tLt \n",
    "        copy!(obs.C, L) # C = L\n",
    "        BLAS.trsm!('R', 'U', 'N', 'N', T(1), obs.storage_qq, obs.C) \n",
    "        BLAS.trsm!('R', 'U', 'T', 'N', T(1), obs.storage_qq, obs.C) \n",
    "        BLAS.trmm!('R', 'L', 'T', 'N', T(1), L, obs.C) \n",
    "        \n",
    "        ## Other common terms that we can pre-compute \n",
    "        ## Calculate Czty\n",
    "        BLAS.gemv!('N', T(1), obs.C, obs.zty, T(0), obs.Czty)\n",
    "        \n",
    "        ## Calculate ztzCzty\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.Czty, T(0), obs.ztzCzty)\n",
    "        \n",
    "        ## Calculate Cztxβ \n",
    "        BLAS.gemv!('N', T(1), obs.C, \n",
    "            BLAS.gemv('N', T(1), obs.ztx, β), T(1), obs.Cztxβ)\n",
    "        \n",
    "        ## Calculate Cztxβ \n",
    "        BLAS.gemv!('N', T(1), obs.C, \n",
    "            BLAS.gemv('N', T(1), obs.ztx, β), T(1), obs.Cztxβ)\n",
    "        \n",
    "        ## Calculate Cztz\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.C, obs.ztz, T(0), obs.Cztz)\n",
    "        \n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        ## Calculate Cztxβ\n",
    "        BLAS.gemv!('N', T(1), obs.ztx, β, T(0), obs.ztxβ)\n",
    "        BLAS.gemv!('N', T(1), obs.C, obs.ztxβ, T(0), obs.Cztxβ)\n",
    "        \n",
    "        ## Calculate ztzCztxβ\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.Cztxβ, T(0), obs.ztzCztxβ)\n",
    "        \n",
    "        ## Calculate ∇β\n",
    "        BLAS.axpby!(T(1 / σ²), obs.xty, T(0), obs.∇β)\n",
    "        BLAS.gemv!('N', T(-1 / σ²), obs.xtx, β, T(1), obs.∇β)\n",
    "        BLAS.gemv!('T', T(-1 / σ²), obs.ztx, obs.Czty, T(1), obs.∇β)\n",
    "        BLAS.gemv!('T', T(1 / σ²), obs.ztx, obs.Cztxβ, T(1), obs.∇β)\n",
=======
    "        ## Calculate ∇β\n",
    "        obs.∇β = (obs.xty - BLAS.gemv('N', T(1), obs.xtx, β) - \n",
    "            BLAS.gemv('T', T(1), obs.ztx, obs.Czty) + \n",
    "            BLAS.gemv('T', T(1), obs.ztx, obs.Cztxβ)) ./ σ²\n",
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
    "        ## Calculate ∇β\n",
    "        obs.∇β = (obs.xty - BLAS.gemv('N', T(1), obs.xtx, β) - \n",
    "            BLAS.gemv('T', T(1), obs.ztx, obs.Czty) + \n",
    "            BLAS.gemv('T', T(1), obs.ztx, obs.Cztxβ)) ./ σ²\n",
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
    "        \n",
    "        ## Calculate ∇σ²\n",
    "        obs.∇σ² = - (n - tr(obs.Cztz)) / (2 * σ²) + abs2(norm(obs.y - \n",
    "                BLAS.gemv('N', T(1), obs.X, β) - \n",
    "                BLAS.gemv('N', T(1), obs.Z, obs.Czty) + \n",
    "                BLAS.gemv('N', T(1), obs.Z, obs.Cztxβ))) / (2 * (σ²)^2)\n",
    "\n",
    "        ## Calculate ∇Σ\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "        BLAS.axpby!(T(1), obs.zty, T(0), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztz, obs.Czty, T(1), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.Cztxβ, T(1), obs.storage_q2)\n",
    "        BLAS.axpby!(T(- 1 / σ²), obs.ztz, T(0), obs.∇Σ)\n",
    "        BLAS.gemm!('N', 'N', T(1 / σ²), obs.ztz, obs.Cztz, T(1), obs.∇Σ)\n",
    "        BLAS.ger!(T(1 / (σ²)^2), obs.storage_q2, obs.storage_q2, obs.∇Σ)\n",
    "        \n",
    "        ## Calculate ∇σ²\n",
    "        tf = dot(obs.Czty, obs.ztzCzty)\n",
    "        tf -= 2dot(obs.ztzCzty, obs.Cztxβ)\n",
    "        tf += dot(obs.Cztxβ, obs.ztzCztxβ)\n",
    "        quad = (rtr - 2qf + tf) / (2 * (σ²)^2)\n",
    "        tra = (n - tr(obs.Cztz)) * (-1 /(2 * σ²))\n",
    "        con = quad + tra\n",
    "        BLAS.axpby!(T(con), ones(1), T(0), obs.∇σ²)\n",
=======
    "        copy!(obs.storage_q2, obs.zty)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztz, obs.Czty, T(1), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.Cztxβ, T(1), obs.storage_q2)\n",
    "        obs.∇Σ = - (obs.ztz - BLAS.gemm(\n",
    "                'N', 'N', T(1), obs.ztz, obs.Cztz)) * (1 / σ²) \n",
    "        BLAS.ger!(T(1 / (σ²)^2), obs.storage_q2, obs.storage_q2, obs.∇Σ)\n",
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
    "        copy!(obs.storage_q2, obs.zty)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztz, obs.Czty, T(1), obs.storage_q2)\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.Cztxβ, T(1), obs.storage_q2)\n",
    "        obs.∇Σ = - (obs.ztz - BLAS.gemm(\n",
    "                'N', 'N', T(1), obs.ztz, obs.Cztz)) * (1 / σ²) \n",
    "        BLAS.ger!(T(1 / (σ²)^2), obs.storage_q2, obs.storage_q2, obs.∇Σ)\n",
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
    "    end    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 232,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": 232,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Correctness"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 224,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": 224,
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, L, σ², true) = -3247.4568580638247\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "obs.∇β = [-1.6309843232725143, -77.52751558041865, -14.702372116010537, 6.978485518989618, -57.71182317682219]\n",
      "obs.∇σ² = [-4.820377758259838]\n",
      "obs.∇Σ = [1.6423791649317923 1.8250240772244597 0.06127650043331857; 1.8250240772244986 0.11072391370533352 0.07213050869972412; 0.061276500433332784 0.07213050869975965 -1.0173748515302994]\n"
=======
      "obs.∇β = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "obs.∇σ² = [0.0]\n",
      "obs.∇Σ = [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n"
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
      "obs.∇β = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "obs.∇σ² = [0.0]\n",
      "obs.∇Σ = [0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0]\n"
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, L, σ², true)\n",
    "@show obs.∇β\n",
    "@show obs.∇σ²\n",
    "@show obs.∇Σ;"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
=======
=======
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: abs(logl - -3247.4568580638247) < 1.0e-8",
     "output_type": "error",
     "traceback": [
      "AssertionError: abs(logl - -3247.4568580638247) < 1.0e-8",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[218]:1"
     ]
    }
   ],
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "source": [
    "@assert abs(logl - (-3247.4568580638247)) < 1e-8\n",
    "@assert norm(obs.∇β - [-1.63098432327115, -77.52751558041871, -14.70237211601065, \n",
    "        6.978485518989568, -57.71182317682199]) < 1e-8\n",
    "@assert norm(obs.∇Σ - [1.6423791649290531 1.82502407722348 0.06127650043330721; \n",
    "        1.82502407722348 0.1107239137055005 0.07213050869971993; \n",
    "        0.06127650043330721 0.07213050869971993 -1.0173748515299939]) < 1e-8\n",
    "@assert abs(obs.∇σ²[1] - (-4.8203777582588145)) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Efficiency"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  96 bytes\n",
       "  allocs estimate:  1\n",
       "  --------------\n",
       "  minimum time:     1.800 μs (0.00% GC)\n",
       "  median time:      1.880 μs (0.00% GC)\n",
       "  mean time:        2.072 μs (0.00% GC)\n",
       "  maximum time:     8.910 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "source": [
    "@benchmark logl!($obs, $β, $L, $σ², false)\n",
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Profile\n",
    "\n",
    "#Profile.clear()\n",
    "#@profile for i in 1:10000; logl!(obs, β, L, σ², true); end\n",
    "#Profile.print(format=:flat, sortedby=:count)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> parent of 2dd2c55... Found the major bottleneck, now trying to solve
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for type stability\n",
    "# @code_warntype logl!(obs, β, L, σ², true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Profile\n",
    "\n",
    "# Profile.clear()\n",
    "# @profile for i in 1:10000; logl!(obs, β, L, σ², true); end\n",
    "# Profile.print(format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: LmmModel Type\n",
    "\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a `LmmModel` object is simply the sum of log-likelihood/gradient of individual data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MathProgBase.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    σ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    ∇β   :: Vector{T}\n",
    "    ∇σ²  :: Vector{T}\n",
    "    ∇L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    σ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    ∇β   = similar(β)    \n",
    "    ∇σ²  = similar(σ²)\n",
    "    ∇L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    LmmModel(obsvec, β, L, σ², ∇β, ∇σ², ∇L, xty, ztr2, xtx, ztz2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.β`, `m.L`, \n",
    "and `m.σ²`. If `needgrad==true`, then `m.∇β`, `m.∇Σ`, and `m.σ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.∇β , 0)\n",
    "        fill!(m.∇L , 0)\n",
    "        fill!(m.∇σ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.β, m.L, m.σ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.∇β, m.∇β)\n",
    "            BLAS.axpy!(T(1), obs.∇Σ, m.∇L)\n",
    "            m.∇σ²[1] += obs.∇σ²[1]\n",
    "        end\n",
    "    end\n",
    "    # obtain gradient wrt L: m.∇L = m.∇L * L\n",
    "    if needgrad\n",
    "       # TODO \n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue)).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(isfile(\"lmm_data.txt\") && filesize(\"lmm_data.txt\") == 246638945) || \n",
    "open(\"lmm_data.txt\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p-1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q-1)\n",
    "        print(io, \"Z\" * string(j) * (j < q-1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:q-1\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1: Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj = logl!(lmm, true) = -2.8547126486833007e6\n",
      "lmm.∇β = [-8.693744360639707, 169.38364540295458, 1412.9462826018269, 583.9807190830802, 57.76586042021151]\n",
      "lmm.∇σ² = [-371.82886426393736]\n",
      "lmm.∇L = [20.19709774963464 -4.566719695055985 9.073934365199074; -5.895609775249732 -13.456093353731692 -18.889943728355036; 12.832481043347801 -20.69289658004678 -61.98953657919886]\n"
     ]
    }
   ],
   "source": [
    "copy!(lmm.β, βtrue)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.σ²[1] = σ²true\n",
    "@show obj = logl!(lmm, true)\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(obj - (-2.854712648683302e6)) < 1e-6\n",
    "@assert norm(lmm.∇β - [-8.693744360651923, 169.38364540290684, \n",
    "        1412.9462826018173, 583.9807190830952, 57.76586042024306]) < 1e-6\n",
    "@assert norm(lmm.∇L - [20.197097749713322 -4.566719695067792 9.073934365205824; \n",
    "        -5.895609775264991 -13.456093353707153 -18.889943728349024; \n",
    "        12.832481043357378 -20.69289658004 -61.98953657919662]) < 1e-6\n",
    "@assert abs(lmm.∇σ²[1] - (-371.8288642639822)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2: Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  93.75 KiB\n",
       "  allocs estimate:  1000\n",
       "  --------------\n",
       "  minimum time:     1.911 ms (0.00% GC)\n",
       "  median time:      2.197 ms (0.00% GC)\n",
       "  mean time:        2.270 ms (0.27% GC)\n",
       "  maximum time:     8.875 ms (75.20% GC)\n",
       "  --------------\n",
       "  samples:          2198\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_model = @benchmark logl!($lmm, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_model).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3: Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 - median(bm_model).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Starting Point\n",
    "\n",
    "For numerical optimization, a good starting point is critical. Let's start $\\boldsymbol{\\beta}$ and $\\sigma^2$ from the least sqaures solutions (ignoring intra-individual correlations),\n",
    "\\begin{eqnarray*}\n",
    "\\boldsymbol{\\beta}^{(0)} &=& \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{X}_i\\right)^{-1} \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{y}_i\\right) \\\\\n",
    "\\sigma^{2(0)} &=& \\frac{\\sum_i \\|\\mathbf{r}_i^{(0)}\\|_2^2}{\\sum_i n_i} = \\frac{\\sum_i \\|\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}^{(0)}\\|_2^2}{\\sum_i n_i}.\n",
    "\\end{eqnarray*}\n",
    "To get a reasonable starting point for $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$, we can minimize the least squares criterion (ignoring the noise variance component),\n",
    "$$\\text{minimize} \\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2,$$\n",
    "Derive the minimizer $\\boldsymbol{\\Sigma}^{(0)}$. We implement this start point strategy in the function `init_ls()`.\n",
    "\n",
    "In order to derive the minimizer, we will want to take the \n",
    "\n",
    "$$\\nabla_{\\Sigma}\\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2 = 0$$\n",
    "\n",
    "First, note that this is a Frobenius norm \n",
    "\n",
    "$$\\nabla_{\\Sigma}\\sum_i Tr\\Bigg(\\Big(\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\Big)\\Big(\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\Big)^T\\Bigg) = 0$$\n",
    "\n",
    "$$\\nabla_{\\Sigma}\\sum_i Tr\\Bigg(\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T}\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T}\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} + \\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\Bigg) = 0$$\n",
    "\n",
    "$$\\nabla_{\\Sigma}\\sum_i Tr\\Big(\\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\Big) - 2\\sum_iTr\\Big(\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T}\\Big) = 0$$\n",
    "\n",
    "Splitting these into two parts we can:\n",
    "\n",
    "$$\\nabla_{\\Sigma}\\sum_i Tr\\Big(\\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{Z}_i^T\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\Big) = \\sum_i 2\\mathbf{Z}_i^T\\mathbf{Z}_i\\boldsymbol{\\Sigma}\\mathbf{Z}_i^T\\mathbf{Z}_i$$\n",
    "\n",
    "$$\\nabla_{\\Sigma}\\sum_iTr\\Big(\\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T}\\Big) =\n",
    "\\nabla_{\\Sigma}\\sum_iTr\\Big(\\mathbf{r}_i^{(0)T} \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)}\\Big) = \\sum_i \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} \\mathbf{Z}_i$$\n",
    "\n",
    "Putting these two terms together we get:\n",
    "\n",
    "$$2\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\boldsymbol{\\Sigma}\\mathbf{Z}_i^T\\mathbf{Z}_i - 2\\sum_i\\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} \\mathbf{Z}_i = 0$$\n",
    "\n",
    "\n",
    "$$\\sum_i\\mathbf{Z}_i^T\\mathbf{Z}_i\\boldsymbol{\\Sigma}\\mathbf{Z}_i^T\\mathbf{Z}_i = \\sum_i\\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} \\mathbf{Z}_i$$\n",
    "\n",
    "Based on properties of outer products leading to Kronecker Products and Vec Operators, we can turn this into: \n",
    "\n",
    "$$\\sum_i \\Big(\\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\otimes \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\Big) = \\sum_i \\Big(\\mathbf{Z}_i^T\\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T\\mathbf{Z}_i \\Big) \\text{vec}\\big(\\mathbf{\\Sigma}\\big)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_ls!"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.β`, `m.L`, and `m.σ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    m.xtx .= T(0)\n",
    "    m.xty .= T(0)\n",
    "    ## Overwrite m.β\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        BLAS.axpby!(T(1), m.data[i].xtx, T(1), m.xtx)\n",
    "        BLAS.axpby!(T(1), m.data[i].xty, T(1), m.xty)\n",
    "    end\n",
    "    LAPACK.potrf!('L', m.xtx)\n",
    "    LAPACK.potrs!('L', m.xtx, m.xty)\n",
    "    copy!(m.β, m.xty)\n",
    "    rsum = T(0)\n",
    "    nsum = T(0)\n",
    "    m.ztr2 .= T(0)\n",
    "    m.ztz2 .= T(0)\n",
    "    # Overwrite m.σ²\n",
    "    @inbounds for i in 1:length(m.data) \n",
    "        obs = m.data[i]\n",
    "        rsum  += obs.yty - 2dot(m.β, obs.xty) + dot(m.β, \n",
    "            BLAS.gemv!('N', T(1), obs.xtx, m.β, T(0), obs.storage_p))\n",
    "        \n",
    "        nsum += size(obs.X, 1)\n",
    "        \n",
    "        BLAS.axpby!(T(1), kron(obs.ztz, obs.ztz), T(1), m.ztz2)\n",
    "\n",
    "        BLAS.gemv!('N', T(1), obs.ztx, m.β, T(0), obs.storage_q)\n",
    "        BLAS.axpby!(T(1), obs.zty, T(-1), obs.storage_q)\n",
    "        BLAS.gemm!('N', 'T', T(1), obs.storage_q, \n",
    "            obs.storage_q, T(0), obs.storage_qq)\n",
    "        BLAS.axpby!(T(1), vec(obs.storage_qq), T(1), m.ztr2)\n",
    "    end     \n",
    "    \n",
    "    BLAS.axpby!(T(rsum / nsum), ones(1), T(0), m.σ²)\n",
    "    LAPACK.potrf!('L', m.ztz2)\n",
    "    LAPACK.potrs!('L', m.ztz2, m.ztr2)\n",
    "    BLAS.axpby!(T(1), reshape(m.ztr2, q, q), T(0), m.L)\n",
    "    LAPACK.potrf!('L', m.L)\n",
    "    m\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl!(lmm) = -3.3750709736056617e6\n",
      "lmm.β = [0.08757868042872419, 6.49874064773743, -3.4970528646610326, 0.9971817478195711, 5.000992653697893]\n",
      "lmm.σ² = [5.6672446612943475]\n",
      "lmm.L = [1.4307910693942878 -0.002995972291969751 0.018911797660621295; -0.002093927168023259 1.08951604771318 -0.012600387416031988; 0.013217721346714463 -0.011539720315817285 0.9605029322102014]\n"
     ]
    }
   ],
   "source": [
    "init_ls!(lmm)\n",
    "@show logl!(lmm)\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1: Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "(logl!(lmm) >  -3.375071e6) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  797.06 KiB\n",
       "  allocs estimate:  3003\n",
       "  --------------\n",
       "  minimum time:     534.100 μs (0.00% GC)\n",
       "  median time:      768.800 μs (0.00% GC)\n",
       "  mean time:        827.548 μs (6.09% GC)\n",
       "  maximum time:     8.652 ms (90.49% GC)\n",
       "  --------------\n",
       "  samples:          6028\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_init = @benchmark init_ls!($lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_init).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: NLP via MathProgBase.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel, solver=Ipopt.IpoptSolver(print_level=5))\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a nonlinear programming solver. Start point \n",
    "should be provided in `m.β`, `m.σ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m::LmmModel,\n",
    "        solver=Ipopt.IpoptSolver(print_level=5)\n",
    "    )\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    npar = p + ((q * (q + 1)) >> 1) + 1\n",
    "    optm = MathProgBase.NonlinearModel(solver)\n",
    "    # set lower bounds and upper bounds of parameters\n",
    "    # diagonal entries of Cholesky factor L should be >= 0\n",
    "    lb   = fill(-Inf, npar)\n",
    "    ub   = fill( Inf, npar)\n",
    "    offset = p + 1\n",
    "    for j in 1:q, i in j:q\n",
    "        i == j && (lb[offset] = 0)\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ² should be >= 0\n",
    "    lb[end] = 0\n",
    "    MathProgBase.loadproblem!(optm, npar, 0, lb, ub, Float64[], Float64[], :Max, m)\n",
    "    # starting point\n",
    "    par0 = zeros(npar)\n",
    "    modelpar_to_optimpar!(par0, m)\n",
    "    MathProgBase.setwarmstart!(optm, par0)\n",
    "    # optimize\n",
    "    MathProgBase.optimize!(optm)\n",
    "    optstat = MathProgBase.status(optm)\n",
    "    optstat == :Optimal || @warn(\"Optimization unsuccesful; got $optstat\")\n",
    "    # update parameters and refresh gradient\n",
    "    optimpar_to_modelpar!(m, MathProgBase.getsolution(optm))\n",
    "    logl!(m, true)\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    modelpar_to_optimpar!(par, m)\n",
    "\n",
    "Translate model parameters in `m` to optimization variables in `par`.\n",
    "\"\"\"\n",
    "function modelpar_to_optimpar!(\n",
    "        par :: Vector,\n",
    "        m   :: LmmModel\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(par, m.β)\n",
    "    # L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        par[offset] = m.L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ²\n",
    "    par[end] = m.σ²[1]\n",
    "    par\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "\n",
    "Translate optimization variables in `par` to the model parameters in `m`.\n",
    "\"\"\"\n",
    "function optimpar_to_modelpar!(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(m.β, 1, par, 1, p)\n",
    "    # L\n",
    "    fill!(m.L, 0)\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        m.L[i, j] = par[offset]\n",
    "        offset   += 1\n",
    "    end\n",
    "    # σ²\n",
    "    m.σ²[1] = par[end]    \n",
    "    m\n",
    "end\n",
    "\n",
    "function MathProgBase.initialize(\n",
    "        m                  :: LmmModel, \n",
    "        requested_features :: Vector{Symbol}\n",
    "    )\n",
    "    for feat in requested_features\n",
    "        if !(feat in [:Grad])\n",
    "            error(\"Unsupported feature $feat\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "MathProgBase.features_available(m::LmmModel) = [:Grad]\n",
    "\n",
    "function MathProgBase.eval_f(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, false) # don't need gradient here\n",
    "end\n",
    "\n",
    "function MathProgBase.eval_grad_f(\n",
    "        m    :: LmmModel, \n",
    "        grad :: Vector, \n",
    "        par  :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    optimpar_to_modelpar!(m, par) \n",
    "    obj = logl!(m, true)\n",
    "    # gradient wrt β\n",
    "    copyto!(grad, m.∇β)\n",
    "    # gradient wrt L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        grad[offset] = m.∇L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # gradient with respect to σ²\n",
    "    grad[end] = m.∇σ²[1]\n",
    "    # return objective\n",
    "    obj\n",
    "end\n",
    "\n",
    "MathProgBase.eval_g(m::LmmModel, g, par) = nothing\n",
    "MathProgBase.jac_structure(m::LmmModel) = Int[], Int[]\n",
    "MathProgBase.eval_jac_g(m::LmmModel, J, par) = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Test Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value at starting point: -3.3750709736056617e6\n",
      "\n",
      "  0.578152 seconds (1.31 M allocations: 65.928 MiB, 1.90% gc time)\n",
      "objective value at solution: -2.8547097852146826e6\n",
      "\n",
      "solution values:\n",
      "lmm.β = [0.08239346903427022, 6.500144748291353, -3.498791185610824, 1.0004995677484871, 5.000049577111127]\n",
      "lmm.σ² = [1.4990426621516175]\n",
      "lmm.L * transpose(lmm.L) = [2.0569432085751163 -0.010097320147474715 0.01850071941170482; -0.010097320147474715 1.1822330517382704 -0.022642859276331195; 0.01850071941170482 -0.022642859276331195 0.937904448641427]\n",
      "gradient @ solution:\n",
      "lmm.∇β = [-0.03412747445660136, -0.08696722104384946, -0.09339139176134381, -0.02887337979338156, -0.06146134036788453]\n",
      "lmm.∇σ² = [0.004134854499170615]\n",
      "lmm.∇L = [-0.023893843049194684 0.08518340486635027 -0.011892084719320791; 0.11106243724093476 0.04151522674661369 -0.05276862367467407; -0.01722977649628616 -0.05926746228986937 0.00024996747443170977]\n",
      "sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + abs2(norm(LowerTriangular(lmm.∇L))))) = 0.15023539817052678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15023539817052678"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "println(\"objective value at starting point: \", logl!(lmm)); println()\n",
    "\n",
    "@time fit!(lmm, NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000));\n",
    "\n",
    "println(\"objective value at solution: \", logl!(lmm)); println()\n",
    "println(\"solution values:\")\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L * transpose(lmm.L)\n",
    "println(\"gradient @ solution:\")\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L\n",
    "@show sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1: Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + abs2(norm(LowerTriangular(lmm.∇L))))) < 0.15",
     "output_type": "error",
     "traceback": [
      "AssertionError: sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + abs2(norm(LowerTriangular(lmm.∇L))))) < 0.15",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[43]:3"
     ]
    }
   ],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert logl!(lmm) > -2.85471e6\n",
    "# gradient at solution should be small enough\n",
    "@assert sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L))))) < 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2: Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  3.96 MiB\n",
       "  allocs estimate:  43291\n",
       "  --------------\n",
       "  minimum time:     119.965 ms (0.00% GC)\n",
       "  median time:      127.534 ms (0.00% GC)\n",
       "  mean time:        132.113 ms (0.13% GC)\n",
       "  maximum time:     171.260 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          38\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_bfgs = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000))) setup = (init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_bfgs).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Gradient free vs gradient-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Optimization unsuccesful; got UserLimit\n",
      "└ @ Main In[41]:34\n"
     ]
    }
   ],
   "source": [
    "# vector of solvers to compare\n",
    "solvers = [\n",
    "    # NLopt: gradient-based algorithms\n",
    "    NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12,\n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12,\n",
    "        maxeval=10000),\n",
    "    NLopt.NLoptSolver(algorithm=:LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # NLopt: gradient-free algorithms\n",
    "    NLopt.NLoptSolver(algorithm=:LN_BOBYQA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # Ipopt\n",
    "    Ipopt.IpoptSolver(print_level=0)\n",
    "]\n",
    "# containers for results\n",
    "runtime = zeros(length(solvers))\n",
    "objvals = zeros(length(solvers))\n",
    "gradnrm = zeros(length(solvers))\n",
    "\n",
    "for (i, solver) in enumerate(solvers)\n",
    "    bm = @benchmark fit!($lmm, $solver) setup = (init_ls!(lmm))\n",
    "    runtime[i] = median(bm).time / 1e9\n",
    "    objvals[i] = logl!(lmm, true)\n",
    "    gradnrm[i] = sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L)))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Runtime</th><th>Objective</th><th>Gradnorm</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>4 rows × 3 columns</p><tr><th>1</th><td>0.139933</td><td>-2.85471e6</td><td>0.150235</td></tr><tr><th>2</th><td>0.147594</td><td>-2.85471e6</td><td>0.000724124</td></tr><tr><th>3</th><td>0.046677</td><td>-2.85603e6</td><td>35185.0</td></tr><tr><th>4</th><td>2.97021</td><td>-2.85471e6</td><td>1.76632e-5</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Runtime & Objective & Gradnorm\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.139933 & -2.85471e6 & 0.150235 \\\\\n",
       "\t2 & 0.147594 & -2.85471e6 & 0.000724124 \\\\\n",
       "\t3 & 0.046677 & -2.85603e6 & 35185.0 \\\\\n",
       "\t4 & 2.97021 & -2.85471e6 & 1.76632e-5 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "4×3 DataFrame\n",
       "│ Row │ Runtime  │ Objective  │ Gradnorm    │\n",
       "│     │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼────────────┼─────────────┤\n",
       "│ 1   │ 0.139933 │ -2.85471e6 │ 0.150235    │\n",
       "│ 2   │ 0.147594 │ -2.85471e6 │ 0.000724124 │\n",
       "│ 3   │ 0.046677 │ -2.85603e6 │ 35185.0     │\n",
       "│ 4   │ 2.97021  │ -2.85471e6 │ 1.76632e-5  │"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(Runtime = runtime, Objective = objvals, Gradnorm = gradnrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Compare with existing art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "method  = [\"257\", \"lme4\", \"MixedModels.jl\"]\n",
    "runtime = zeros(3)  # record the run times\n",
    "loglike = zeros(3); # record the log-likelihood at MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1: Your Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8547097852035607e6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_257 = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm=:LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000))) setup=(init_ls!(lmm))\n",
    "runtime[1] = (median(bm_257).time) / 1e9\n",
    "loglike[1] = logl!(lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2: lme4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "|                                                                        |   0%\r",
      "|                                                                        |   0%\r",
      "|                                                                        |   0%\r",
      "|                                                                        |   0%\r",
      "|                                                                        |   0%\r",
      "|                                                                        |   0%\r",
      "|                                                                |   0%    1 MB\r",
      "|                                                                |   0%    1 MB\r",
      "|                                                                |   0%    1 MB\r",
      "|                                                                |   0%    1 MB\r",
      "|                                                                |   0%    1 MB\r",
      "|                                                                |   0%    1 MB\r",
      "|                                                                |   0%    2 MB\r",
      "|                                                                |   0%    2 MB\r",
      "|                                                                |   1%    2 MB\r",
      "|                                                                |   1%    2 MB\r",
      "|                                                                |   1%    2 MB\r",
      "|                                                                |   1%    2 MB\r",
      "|                                                                |   1%    3 MB\r",
      "|                                                                |   1%    3 MB\r",
      "|                                                                |   1%    3 MB\r",
      "|=                                                               |   1%    3 MB\r",
      "|=                                                               |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Loading required package: Matrix\n",
      "└ @ RCall C:\\Users\\jbola\\.julia\\packages\\RCall\\Qzssx\\src\\io.jl:160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=================================================================| 99%  235 MB|   5%   11 MB|   8%   20 MB|  10%   25 MB========                                                        |  13%   30 MB|  14%   33 MB|  15%   36 MB  16%   39 MB|  19%   46 MB  29%   68 MB====================                                            |  31%   73 MB|  33%   78 MB  42%  100 MB  46%  109 MB  54%  129 MB|  62%  146 MB  64%  150 MB|  66%  155 MB  70%  166 MB                 |  72%  171 MB  76%  180 MB  78%  183 MB  82%  193 MB          |  83%  196 MB  84%  197 MB=======================================================         |  85%  201 MB|  85%  201 MB  87%  205 MB  88%  208 MB      |  89%  211 MB  90%  212 MB  91%  214 MB|  93%  220 MB=============================================================   |  94%  221 MB|  95%  225 MB=============================================================== |  97%  229 MB 100%  235 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Parsed with column specification:\n",
      "│ cols(\n",
      "│   ID = col_double(),\n",
      "│   Y = col_double(),\n",
      "│   X1 = col_double(),\n",
      "│   X2 = col_double(),\n",
      "│   X3 = col_double(),\n",
      "│   X4 = col_double(),\n",
      "│   Z1 = col_double(),\n",
      "│   Z2 = col_double()\n",
      "│ )\n",
      "└ @ RCall C:\\Users\\jbola\\.julia\\packages\\RCall\\Qzssx\\src\\io.jl:160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{VecSxp}\n",
       "# A tibble: 1,753,910 x 8\n",
       "      ID      Y      X1      X2     X3      X4      Z1     Z2\n",
       "   <dbl>  <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>  <dbl>\n",
       " 1     1   2.30  2.16    1.71    1.95  -0.744   0.441  -0.155\n",
       " 2     1  -7.82  0.591   0.0736 -1.39  -1.53   -0.284  -0.283\n",
       " 3     1  -1.05 -0.666   0.223  -1.07   0.726  -0.701  -0.639\n",
       " 4     1  -5.42 -0.364   0.181   1.61  -0.623   0.0332  0.905\n",
       " 5     1  -4.23  0.880   1.91   -0.999 -0.371   1.03    0.581\n",
       " 6     1  -2.72  0.341   0.162   0.603 -0.285   1.42   -0.831\n",
       " 7     1   1.13 -0.0213 -0.500   0.178  0.747   0.638  -0.617\n",
       " 8     1 -17.6  -1.76    0.899  -0.931 -0.0527  0.353  -1.38 \n",
       " 9     1  -4.30 -0.207   0.248  -0.362 -0.0995 -0.587   0.266\n",
       "10     1  20.9   2.24    0.523  -1.47   2.07    0.714   2.06 \n",
       "# ... with 1,753,900 more rows\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "library(lme4)\n",
    "library(readr)\n",
    "library(magrittr)\n",
    "\n",
    "testdata <- read_csv(\"lmm_data.txt\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,  :\n",
      "│   Model failed to converge with max|grad| = 0.00229032 (tol = 0.002, component 1)\n",
      "└ @ RCall C:\\Users\\jbola\\.julia\\packages\\RCall\\Qzssx\\src\\io.jl:160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{RealSxp}\n",
       "   user  system elapsed \n",
       "  90.80   20.67  110.93 \n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "rtime <- system.time(mmod <- \n",
    "  lmer(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID), testdata, REML = FALSE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- rtime[\"elapsed\"]\n",
    "summary(mmod)\n",
    "rlogl <- logLik(mmod)\n",
    "\"\"\"\n",
    "runtime[2] = @rget rtime\n",
    "loglike[2] = @rget rlogl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3: MixedModels.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>ID</th><th>Y</th><th>X1</th><th>X2</th><th>X3</th><th>X4</th><th>Z1</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>1,753,910 rows × 8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>1</td><td>2.29562</td><td>2.16439</td><td>1.71386</td><td>1.94501</td><td>-0.743783</td><td>0.441018</td></tr><tr><th>2</th><td>1</td><td>-7.82426</td><td>0.591156</td><td>0.0735725</td><td>-1.38799</td><td>-1.53388</td><td>-0.28432</td></tr><tr><th>3</th><td>1</td><td>-1.05162</td><td>-0.665533</td><td>0.222603</td><td>-1.06705</td><td>0.726237</td><td>-0.700676</td></tr><tr><th>4</th><td>1</td><td>-5.41677</td><td>-0.363982</td><td>0.181063</td><td>1.6054</td><td>-0.623457</td><td>0.0332452</td></tr><tr><th>5</th><td>1</td><td>-4.22634</td><td>0.880121</td><td>1.91223</td><td>-0.998628</td><td>-0.370723</td><td>1.02984</td></tr><tr><th>6</th><td>1</td><td>-2.72221</td><td>0.341423</td><td>0.161899</td><td>0.602739</td><td>-0.285159</td><td>1.4209</td></tr><tr><th>7</th><td>1</td><td>1.13149</td><td>-0.0212729</td><td>-0.500254</td><td>0.177658</td><td>0.747107</td><td>0.638381</td></tr><tr><th>8</th><td>1</td><td>-17.6405</td><td>-1.76199</td><td>0.898998</td><td>-0.931399</td><td>-0.0526501</td><td>0.352804</td></tr><tr><th>9</th><td>1</td><td>-4.30473</td><td>-0.206732</td><td>0.248303</td><td>-0.362355</td><td>-0.0994689</td><td>-0.58678</td></tr><tr><th>10</th><td>1</td><td>20.9383</td><td>2.23638</td><td>0.523116</td><td>-1.46907</td><td>2.07343</td><td>0.714249</td></tr><tr><th>11</th><td>1</td><td>-4.59161</td><td>-1.02311</td><td>1.66654</td><td>0.766974</td><td>1.85919</td><td>2.02149</td></tr><tr><th>12</th><td>1</td><td>0.836847</td><td>0.676822</td><td>-0.0889856</td><td>-1.49619</td><td>-0.539135</td><td>0.0793583</td></tr><tr><th>13</th><td>1</td><td>-5.84599</td><td>-0.397877</td><td>-0.450184</td><td>0.349735</td><td>-0.542674</td><td>-0.565936</td></tr><tr><th>14</th><td>1</td><td>-6.23177</td><td>-2.0327</td><td>-0.608492</td><td>-0.0404538</td><td>0.88565</td><td>-1.67842</td></tr><tr><th>15</th><td>1</td><td>-5.9369</td><td>-1.38549</td><td>-1.80727</td><td>-1.31258</td><td>-0.0271809</td><td>2.2038</td></tr><tr><th>16</th><td>1</td><td>-5.32273</td><td>-0.61431</td><td>-0.0859859</td><td>0.970478</td><td>-0.0508639</td><td>2.38902</td></tr><tr><th>17</th><td>1</td><td>0.0758345</td><td>-0.122086</td><td>-2.02785</td><td>-0.161972</td><td>-0.805525</td><td>0.3504</td></tr><tr><th>18</th><td>1</td><td>7.52945</td><td>-0.298849</td><td>-2.7182</td><td>-1.06486</td><td>0.755002</td><td>-0.664704</td></tr><tr><th>19</th><td>1</td><td>-0.580833</td><td>0.0263333</td><td>-0.432225</td><td>-0.987936</td><td>0.0336734</td><td>-0.14892</td></tr><tr><th>20</th><td>1</td><td>-13.5485</td><td>-0.00237514</td><td>1.04502</td><td>-0.950582</td><td>-1.25265</td><td>-1.27643</td></tr><tr><th>21</th><td>1</td><td>-3.42085</td><td>-0.671995</td><td>-0.272527</td><td>-0.22161</td><td>0.18748</td><td>1.99234</td></tr><tr><th>22</th><td>1</td><td>-0.398223</td><td>0.939217</td><td>0.158931</td><td>0.211848</td><td>-0.627386</td><td>0.684831</td></tr><tr><th>23</th><td>1</td><td>-3.78251</td><td>-0.145142</td><td>-0.860611</td><td>0.143043</td><td>-0.534395</td><td>1.50549</td></tr><tr><th>24</th><td>1</td><td>5.18177</td><td>1.26779</td><td>0.440199</td><td>-0.342523</td><td>0.264829</td><td>0.979096</td></tr><tr><th>25</th><td>1</td><td>-10.2052</td><td>-0.839727</td><td>-0.00752814</td><td>1.03254</td><td>-1.02572</td><td>1.27587</td></tr><tr><th>26</th><td>1</td><td>3.2676</td><td>0.303675</td><td>0.398647</td><td>-2.24835</td><td>0.977578</td><td>-0.520845</td></tr><tr><th>27</th><td>1</td><td>6.64642</td><td>-0.259489</td><td>-0.822341</td><td>-0.488534</td><td>1.42375</td><td>-0.445779</td></tr><tr><th>28</th><td>1</td><td>6.98399</td><td>0.789707</td><td>-1.79072</td><td>-0.423215</td><td>-0.566674</td><td>0.176244</td></tr><tr><th>29</th><td>1</td><td>-16.3496</td><td>-0.735116</td><td>1.75313</td><td>-1.24459</td><td>-0.984253</td><td>-0.930345</td></tr><tr><th>30</th><td>1</td><td>-4.59682</td><td>-0.25318</td><td>0.0998096</td><td>0.682749</td><td>-0.141295</td><td>-1.47255</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& ID & Y & X1 & X2 & X3 & X4 & Z1 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 2.29562 & 2.16439 & 1.71386 & 1.94501 & -0.743783 & 0.441018 & $\\dots$ \\\\\n",
       "\t2 & 1 & -7.82426 & 0.591156 & 0.0735725 & -1.38799 & -1.53388 & -0.28432 & $\\dots$ \\\\\n",
       "\t3 & 1 & -1.05162 & -0.665533 & 0.222603 & -1.06705 & 0.726237 & -0.700676 & $\\dots$ \\\\\n",
       "\t4 & 1 & -5.41677 & -0.363982 & 0.181063 & 1.6054 & -0.623457 & 0.0332452 & $\\dots$ \\\\\n",
       "\t5 & 1 & -4.22634 & 0.880121 & 1.91223 & -0.998628 & -0.370723 & 1.02984 & $\\dots$ \\\\\n",
       "\t6 & 1 & -2.72221 & 0.341423 & 0.161899 & 0.602739 & -0.285159 & 1.4209 & $\\dots$ \\\\\n",
       "\t7 & 1 & 1.13149 & -0.0212729 & -0.500254 & 0.177658 & 0.747107 & 0.638381 & $\\dots$ \\\\\n",
       "\t8 & 1 & -17.6405 & -1.76199 & 0.898998 & -0.931399 & -0.0526501 & 0.352804 & $\\dots$ \\\\\n",
       "\t9 & 1 & -4.30473 & -0.206732 & 0.248303 & -0.362355 & -0.0994689 & -0.58678 & $\\dots$ \\\\\n",
       "\t10 & 1 & 20.9383 & 2.23638 & 0.523116 & -1.46907 & 2.07343 & 0.714249 & $\\dots$ \\\\\n",
       "\t11 & 1 & -4.59161 & -1.02311 & 1.66654 & 0.766974 & 1.85919 & 2.02149 & $\\dots$ \\\\\n",
       "\t12 & 1 & 0.836847 & 0.676822 & -0.0889856 & -1.49619 & -0.539135 & 0.0793583 & $\\dots$ \\\\\n",
       "\t13 & 1 & -5.84599 & -0.397877 & -0.450184 & 0.349735 & -0.542674 & -0.565936 & $\\dots$ \\\\\n",
       "\t14 & 1 & -6.23177 & -2.0327 & -0.608492 & -0.0404538 & 0.88565 & -1.67842 & $\\dots$ \\\\\n",
       "\t15 & 1 & -5.9369 & -1.38549 & -1.80727 & -1.31258 & -0.0271809 & 2.2038 & $\\dots$ \\\\\n",
       "\t16 & 1 & -5.32273 & -0.61431 & -0.0859859 & 0.970478 & -0.0508639 & 2.38902 & $\\dots$ \\\\\n",
       "\t17 & 1 & 0.0758345 & -0.122086 & -2.02785 & -0.161972 & -0.805525 & 0.3504 & $\\dots$ \\\\\n",
       "\t18 & 1 & 7.52945 & -0.298849 & -2.7182 & -1.06486 & 0.755002 & -0.664704 & $\\dots$ \\\\\n",
       "\t19 & 1 & -0.580833 & 0.0263333 & -0.432225 & -0.987936 & 0.0336734 & -0.14892 & $\\dots$ \\\\\n",
       "\t20 & 1 & -13.5485 & -0.00237514 & 1.04502 & -0.950582 & -1.25265 & -1.27643 & $\\dots$ \\\\\n",
       "\t21 & 1 & -3.42085 & -0.671995 & -0.272527 & -0.22161 & 0.18748 & 1.99234 & $\\dots$ \\\\\n",
       "\t22 & 1 & -0.398223 & 0.939217 & 0.158931 & 0.211848 & -0.627386 & 0.684831 & $\\dots$ \\\\\n",
       "\t23 & 1 & -3.78251 & -0.145142 & -0.860611 & 0.143043 & -0.534395 & 1.50549 & $\\dots$ \\\\\n",
       "\t24 & 1 & 5.18177 & 1.26779 & 0.440199 & -0.342523 & 0.264829 & 0.979096 & $\\dots$ \\\\\n",
       "\t25 & 1 & -10.2052 & -0.839727 & -0.00752814 & 1.03254 & -1.02572 & 1.27587 & $\\dots$ \\\\\n",
       "\t26 & 1 & 3.2676 & 0.303675 & 0.398647 & -2.24835 & 0.977578 & -0.520845 & $\\dots$ \\\\\n",
       "\t27 & 1 & 6.64642 & -0.259489 & -0.822341 & -0.488534 & 1.42375 & -0.445779 & $\\dots$ \\\\\n",
       "\t28 & 1 & 6.98399 & 0.789707 & -1.79072 & -0.423215 & -0.566674 & 0.176244 & $\\dots$ \\\\\n",
       "\t29 & 1 & -16.3496 & -0.735116 & 1.75313 & -1.24459 & -0.984253 & -0.930345 & $\\dots$ \\\\\n",
       "\t30 & 1 & -4.59682 & -0.25318 & 0.0998096 & 0.682749 & -0.141295 & -1.47255 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "1753910×8 DataFrame. Omitted printing of 3 columns\n",
       "│ Row     │ ID     │ Y        │ X1         │ X2         │ X3        │\n",
       "│         │ \u001b[90mString\u001b[39m │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────────┼────────┼──────────┼────────────┼────────────┼───────────┤\n",
       "│ 1       │ 1      │ 2.29562  │ 2.16439    │ 1.71386    │ 1.94501   │\n",
       "│ 2       │ 1      │ -7.82426 │ 0.591156   │ 0.0735725  │ -1.38799  │\n",
       "│ 3       │ 1      │ -1.05162 │ -0.665533  │ 0.222603   │ -1.06705  │\n",
       "│ 4       │ 1      │ -5.41677 │ -0.363982  │ 0.181063   │ 1.6054    │\n",
       "│ 5       │ 1      │ -4.22634 │ 0.880121   │ 1.91223    │ -0.998628 │\n",
       "│ 6       │ 1      │ -2.72221 │ 0.341423   │ 0.161899   │ 0.602739  │\n",
       "│ 7       │ 1      │ 1.13149  │ -0.0212729 │ -0.500254  │ 0.177658  │\n",
       "│ 8       │ 1      │ -17.6405 │ -1.76199   │ 0.898998   │ -0.931399 │\n",
       "│ 9       │ 1      │ -4.30473 │ -0.206732  │ 0.248303   │ -0.362355 │\n",
       "│ 10      │ 1      │ 20.9383  │ 2.23638    │ 0.523116   │ -1.46907  │\n",
       "⋮\n",
       "│ 1753900 │ 1000   │ 2.34091  │ 1.49228    │ 0.682125   │ 0.302774  │\n",
       "│ 1753901 │ 1000   │ -3.18819 │ -0.689118  │ -0.612859  │ 0.0738376 │\n",
       "│ 1753902 │ 1000   │ -11.3465 │ 0.763788   │ 2.90449    │ 0.162727  │\n",
       "│ 1753903 │ 1000   │ 8.6989   │ 1.51636    │ 0.137675   │ 0.308407  │\n",
       "│ 1753904 │ 1000   │ -18.9671 │ 0.145233   │ 1.34346    │ -2.42149  │\n",
       "│ 1753905 │ 1000   │ -0.1842  │ -0.916041  │ -0.586905  │ 0.77127   │\n",
       "│ 1753906 │ 1000   │ -6.04537 │ -1.28779   │ 0.165535   │ 0.24736   │\n",
       "│ 1753907 │ 1000   │ 2.55843  │ -0.19814   │ -0.0349219 │ 0.48918   │\n",
       "│ 1753908 │ 1000   │ 3.49073  │ -0.0606105 │ 0.199115   │ 1.30314   │\n",
       "│ 1753909 │ 1000   │ 1.58389  │ -0.716483  │ -0.864964  │ 0.0262824 │\n",
       "│ 1753910 │ 1000   │ 2.16344  │ -0.889437  │ -0.966382  │ -0.842096 │"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = CSV.File(\"lmm_data.txt\", types = Dict(1=>String)) |> DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0497932"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mj = fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "bm_mm = @benchmark fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "loglike[3] = loglikelihood(mj)\n",
    "runtime[3] = median(bm_mm).time / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  918.49 MiB\n",
       "  allocs estimate:  9397717\n",
       "  --------------\n",
       "  minimum time:     986.031 ms (10.96% GC)\n",
       "  median time:      1.050 s (10.29% GC)\n",
       "  mean time:        1.060 s (11.90% GC)\n",
       "  maximum time:     1.155 s (14.45% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " Y ~ 1 + X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)\n",
       "     logLik        -2 logLik          AIC             BIC       \n",
       " -2.85470979×10⁶  5.70941957×10⁶  5.70944357×10⁶   5.7095921×10⁶\n",
       "\n",
       "Variance components:\n",
       "            Column    Variance   Std.Dev.   Corr.\n",
       "ID       (Intercept)  2.05673781 1.4341331\n",
       "         Z1           1.18229376 1.0873333 -0.01\n",
       "         Z2           0.93787178 0.9684378  0.01 -0.02\n",
       "Residual              1.49904276 1.2243540\n",
       " Number of obs: 1753910; levels of grouping factors: 1000\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "──────────────────────────────────────────────────────────\n",
       "               Estimate    Std.Error      z value  P(>|z|)\n",
       "──────────────────────────────────────────────────────────\n",
       "(Intercept)   0.0823234  0.0453559        1.81505   0.0695\n",
       "X1            6.50014    0.000924999   7027.19      <1e-99\n",
       "X2           -3.49879    0.000924895  -3782.91      <1e-99\n",
       "X3            1.0005     0.000925797   1080.69      <1e-99\n",
       "X4            5.00005    0.000925235   5404.09      <1e-99\n",
       "──────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(bm_mm)\n",
    "mj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>method</th><th>runtime</th><th>logl</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows × 3 columns</p><tr><th>1</th><td>257</td><td>0.136062</td><td>-2.85471e6</td></tr><tr><th>2</th><td>lme4</td><td>110.93</td><td>-2.85471e6</td></tr><tr><th>3</th><td>MixedModels.jl</td><td>1.04979</td><td>-2.85471e6</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& method & runtime & logl\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 257 & 0.136062 & -2.85471e6 \\\\\n",
       "\t2 & lme4 & 110.93 & -2.85471e6 \\\\\n",
       "\t3 & MixedModels.jl & 1.04979 & -2.85471e6 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3×3 DataFrame\n",
       "│ Row │ method         │ runtime  │ logl       │\n",
       "│     │ \u001b[90mString\u001b[39m         │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m    │\n",
       "├─────┼────────────────┼──────────┼────────────┤\n",
       "│ 1   │ 257            │ 0.136062 │ -2.85471e6 │\n",
       "│ 2   │ lme4           │ 110.93   │ -2.85471e6 │\n",
       "│ 3   │ MixedModels.jl │ 1.04979  │ -2.85471e6 │"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(method = method, runtime = runtime, logl = loglike)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
