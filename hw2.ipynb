{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOSTAT 257 Homework 2\n",
    "\n",
    "Consider a linear mixed effects model\n",
    "\n",
    "$$Y_i = X_i\\beta + Z_i\\gamma + \\epsilon_i, \\quad i = 1,\\ldots,n$$\n",
    "\n",
    "where\n",
    "\n",
    "- $Y_i \\in \\mathbb{R}^{n_i}$ is the reponse vector of the $i$-th individual,\n",
    "- $X_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effect predictor matrix of  $i$-th individual,\n",
    "- $Z_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effect predictor matrix of  $i$-th individual, \n",
    "- $\\epsilon_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(0_{n_i}, \\sigma^2 I_{n_i})$,\n",
    "- $\\beta \\in \\mathbb{R}^{p}$ are fixed effects, and\n",
    "- $\\gamma \\in \\mathbb{R}^{q}$ are random effects assumed to be $N(0_{q}, \\Sigma_{q \\times q})$ independent of $\\epsilon_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Formula\n",
    "\n",
    "Write down the log-likelihood of the  $i$-th datum  $(Y_i,X_i,Z_i)$  given parameters $(\\beta,\\Sigma,\\sigma^2)$.\n",
    "\n",
    "The marginal distribution of $Y_i \\sim N(X_i \\beta, Z_i \\Sigma Z_i^T + \\sigma^2 I_{n_i})$\n",
    "\n",
    "$$\\ell(\\beta,\\Sigma,\\sigma^2) = -\\frac{n_i}{2}\\text{log}(2\\pi) - \\frac{1}{2}\\text{log}|Z_i \\Sigma Z_i^T + \\sigma^2 I_{n_i}| - \\frac{1}{2}(Y_i - X_i \\beta)^T(Z_i \\Sigma Z_i^T + \\sigma^2 I_{n_i})^{-1}(Y_i - X_i \\beta)$$\n",
    "\n",
    "The most computationally challenging terms will be the log determinant and the matrix inversion. First we know that $\\Sigma = L L'$. We can write \n",
    "\n",
    "$$Z\\Sigma Z^T = Z L L^T Z^T = (ZL)(ZL)^T = RR^T$$\n",
    "\n",
    "We can use Woodbury to rewrite the covariance matrix as:\n",
    "\n",
    "$$(RR^T + \\sigma^2 I_{n_i})^{-1} = \\frac{1}{\\sigma^2}I -\\frac{1}{\\sigma^4}R\\Bigg(I + \\frac{1}{\\sigma^2}R^TR\\Bigg)^{-1}R^T.$$\n",
    "\n",
    "Then, the quadratic form then becomes: \n",
    "\n",
    "$$\\frac{1}{\\sigma^2}(y-X\\beta)^T(y-X\\beta) -\\frac{1}{\\sigma^4}(y-X\\beta)^TR\\Bigg(I + \\frac{1}{\\sigma^2}R^TR\\Bigg)^{-1}R^T(y-X\\beta)$$\n",
    "\n",
    "where if we let $C = R^T(y-X\\beta) = L^TZ^T(y-X\\beta)$, this then becomes:\n",
    "\n",
    "$$\\frac{1}{\\sigma^2}(y-X\\beta)^T(y-X\\beta) -\\frac{1}{\\sigma^4}C^T\\Bigg(I + \\frac{1}{\\sigma^2}R^TR\\Bigg)^{-1}C$$\n",
    "\n",
    "$$I + \\frac{1}{\\sigma^2}R^TR = MM^T.$$\n",
    "\n",
    "Which leads us to: \n",
    "\n",
    "$$(RR^T + \\sigma^2 I_{n_i})^{-1} = \\frac{1}{\\sigma^2}I -\\frac{1}{\\sigma^4}R\\Bigg(MM^T\\Bigg)^{-1}R^T = \\frac{1}{\\sigma^2}I -\\frac{1}{\\sigma^4}RM^{-T}M^{-1}R^T$$\n",
    "\n",
    "\n",
    "$$(RR^T + \\sigma^2 I_{n_i})^{-1} = \\frac{1}{\\sigma^2}I -\\frac{1}{\\sigma^4}Q^TQ$$\n",
    "\n",
    "such that $M^{-1}R^T = Q$. If we let $e = y - X \\beta$, then we can write the quadratic form as:\n",
    "\n",
    "$$e^T(\\frac{1}{\\sigma^2}I -\\frac{1}{\\sigma^4}Q^TQ)e = \\frac{1}{\\sigma^2}e^Te - \\frac{1}{\\sigma^4} e^TQ^TQe$$\n",
    "\n",
    "The other difficult term to work with is \n",
    "\n",
    "$$\\text{det}(\\sigma^2I + Z\\Sigma Z^T) = \\text{det}(\\sigma^2I)\\text{det}\\Bigg(I + \\frac{1}{\\sigma^2}R^TR\\Bigg) = (\\sigma^2)^n\\text{det}(MM^T)$$\n",
    "\n",
    "Thus we can re-write the log-likelihood as\n",
    "\n",
    "$$\\ell(\\beta,\\Sigma,\\sigma^2) = -\\frac{n_i}{2}\\text{log}(2\\pi) - \\frac{1}{2}log((\\sigma^2)^n\\text{det}(MM^T)) - \\frac{1}{2\\sigma^2}e^Te + \\frac{1}{2\\sigma^4} e^TQ^TQe$$\n",
    "\n",
    "$$\\ell(\\beta,\\Sigma,\\sigma^2) = -\\frac{n_i}{2}\\text{log}(2\\pi) - \\frac{n_i}{2}log(\\sigma^2) -\\frac{1}{2}log(\\text{det}(MM^T)) - \\frac{1}{2\\sigma^2}e^Te + \\frac{1}{2\\sigma^4} e^TQ^TQe$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Start-up Code\n",
    "\n",
    "Use the following template to define a type `LmmObs` that holds an LMM datum $(y_i,X_i,Z_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "    res         :: Vector{T}\n",
    "    storage_q   :: Vector{T}\n",
    "    storage_q2  :: Vector{T}\n",
    "    ztz         :: Matrix{T}\n",
    "    storage_qq  :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "    res         = similar(y)\n",
    "    storage_q   = Vector{T}(undef, size(Z, 2))\n",
    "    storage_q2  = Vector{T}(undef, size(Z, 2))\n",
    "    ztz         = transpose(Z) * Z\n",
    "    storage_qq  = similar(ztz)\n",
    "    storage_qq2 = similar(ztz)\n",
    "    LmmObs(y, X, Z, res, storage_q, storage_q2, ztz, storage_qq, storage_qq2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, with interface `logl!(obs, β, L, σ²)` that evaluates the log-likelihood of the $i$-th datum. Here `L` is the lower triangular Cholesky factor from the Cholesky decomposition `Σ=LL'`. Make your code efficient in the $n_i≫q$ case. Think the intensive longitudinal measurement setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random\n",
    "\n",
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2) \n",
    "    ## Calculate y - Xβ\n",
    "    mul!(obs.res, obs.X, β)\n",
    "    axpy!(-1, obs.y, obs.res)\n",
    "    \n",
    "    ## Start calculating (I + (1/σ^2)R^tR)\n",
    "    mul!(obs.storage_qq, obs.ztz, L)\n",
    "    mul!(obs.storage_qq2, L', obs.storage_qq)\n",
    "    mul!(obs.ztz, obs.storage_qq2, (1/σ²))\n",
    "    for i = 1:q\n",
    "        obs.ztz[i, i] += 1\n",
    "    end\n",
    "    \n",
    "    ## Cholesky Decomposition of (I + (1/σ^2)R^tR)\n",
    "    cholesky!(Symmetric(obs.ztz))\n",
    "\n",
    "    ## M^-1L^tZ^T(y-Xβ)\n",
    "    mul!(obs.storage_q, obs.Z', obs.res)\n",
    "    mul!(obs.storage_q2, L', obs.storage_q)\n",
    "    ldiv!(obs.storage_q, obs.ztz, obs.storage_q2)\n",
    "    \n",
    "    l = -(n/2) * log(2 * π * σ²)  - \n",
    "        (1/2) * logdet(obs.ztz) - (1/(2 * σ²)) * BLAS.dot(obs.res, obs.res) + \n",
    "        (1/(2 * (σ²)^2)) * BLAS.dot(obs.storage_q2, obs.storage_q)\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: This function shouldn't be very long. Mine, obeying 80-character rule, is 25 lines. If you find yourself writing very long code, you're on the wrong track. Think about algorithm first then use BLAS functions to reduce memory allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Correctness\n",
    "\n",
    "Compare your result (both accuracy and timing) to the Distributions.jl package using following data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs{Float64}([5.739048710854997, 5.705395720270055, 2.7368899643050355, 1.4201223592870755, -0.2099433929180451, 3.5886971824690486, -1.3778538474575956, -0.08406026821055246, -2.208007878450787, 1.309558511583542  …  1.2947876180172684, -1.9701265304395086, -2.040383092851745, -1.4590296825658675, 0.18616271231054726, 1.0681247149968018, 2.2292080864625254, 1.1952385354603545, 1.1310626949609701, -0.43507816286713785], [1.0 -2.506566300781151 … 0.5863780184080776 1.1092991040518192; 1.0 -0.974090320735282 … 1.4143507320583761 0.45608259198567447; … ; 1.0 -1.0076371084863895 … -1.3241972696483915 1.4547609424344008; 1.0 0.38036793320364776 … -0.5857507269707397 1.796804266836504], [1.0 -0.6380567326757537 1.4738982136806946; 1.0 -2.0711110232845926 0.21422658785510312; … ; 1.0 0.5917731507133951 -0.9163364468263059; 1.0 0.9463732120394507 -0.325860403600768], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.504983e-316, 5.37417495e-316, 5.36517505e-316], [0.0, 1049.0009765625002, 0.0], [2000.0 -11.2035856885878 -23.35638533913959; -11.2035856885878 1972.7426082447305 27.303296982632173; -23.35638533913959 27.303296982632173 2034.203494486357], [7.7634266e-316 1.41009426e-315 1.410095052e-315; 1.41009379e-315 1.41009458e-315 1.41009521e-315; 1.410093946e-315 1.410094736e-315 1.41009537e-315], [7.7634266e-316 1.41009426e-315 1.410095052e-315; 1.41009379e-315 1.41009458e-315 1.41009521e-315; 1.410093946e-315 1.410094736e-315 1.41009537e-315])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard way to evaluate log-density of a multivariate normal, using the Distributions.jl package. Let's evaluate the log-likelihood of this datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.456858063827"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your answer matches that from Distributions.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3247.4568580638297"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "logl!(obs, β, L, σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 15 + 30 + 30 = 75 points if the following statement throws AssertError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: logl!(obs, β, Matrix((cholesky(Σ)).L), σ²) ≈ logpdf(mvn, y)",
     "output_type": "error",
     "traceback": [
      "AssertionError: logl!(obs, β, Matrix((cholesky(Σ)).L), σ²) ≈ logpdf(mvn, y)",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[88]:1"
     ]
    }
   ],
   "source": [
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Efficiency\n",
    "\n",
    "Benchmarking your code and compare to the Distributions.jl function `logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  30.55 MiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     11.848 ms (0.00% GC)\n",
       "  median time:      12.461 ms (0.00% GC)\n",
       "  mean time:        14.356 ms (12.24% GC)\n",
       "  maximum time:     31.173 ms (49.15% GC)\n",
       "  --------------\n",
       "  samples:          349\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the `logpdf` function in Distribution.jl\n",
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  272 bytes\n",
       "  allocs estimate:  7\n",
       "  --------------\n",
       "  minimum time:     36.100 μs (0.00% GC)\n",
       "  median time:      43.000 μs (0.00% GC)\n",
       "  mean time:        44.868 μs (0.00% GC)\n",
       "  maximum time:     444.101 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points you will get is\n",
    "$$\\frac{x}{1000} \\times 30,$$\n",
    "\n",
    "where $x$ is the speedup of your program against the standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.693930930232558"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you'll get\n",
    "clamp(median(bm1).time / median(bm2).time / 1000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Apparently I am using 1000 as denominator because I expect your code to be at least  1000×  faster than the standard method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Memory\n",
    "\n",
    "You want to avoid memory allocation in the \"hot\" function `logl!`. You will lose 1 point for each `1 KiB = 1024 bytes` memory allocation. In other words, the points you get for this question is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.734375"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: I am able to reduce the memory allocation to 0 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Misc.\n",
    "\n",
    "Coding style, Git workflow, etc. For reproducibity, make sure we (TA and myself) can run your Jupyter Notebook. That is how we grade Q4 and Q5. If we cannot run it, you will get zero points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
