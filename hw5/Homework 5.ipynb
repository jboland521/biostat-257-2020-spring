{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOSTAT 257: Homework 5\n",
    "### Joanna Boland\n",
    "\n",
    "Again we continue with the linear mixed effects model (LMM)\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma_0^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T.\n",
    "$$\n",
    "Given $m$ independent data points $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma_0^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma_0^2).\n",
    "$$\n",
    "\n",
    "In HW4, we used the nonlinear programming (NLP) approach (Newton type algorithms) for optimization. In this assignment, we derive and implement an expectation-maximization (EM) algorithm for the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, Distributions, LinearAlgebra, Random, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Refresher on Normal-Normal Model\n",
    "\n",
    "Assume the conditional distribution\n",
    "$$\n",
    "\\mathbf{y} \\mid \\boldsymbol{\\gamma} \\sim N(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma}, \\sigma^2 \\mathbf{I}_n)\n",
    "$$\n",
    "and the prior distribution\n",
    "$$\n",
    "\\boldsymbol{\\gamma} \\sim N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}).\n",
    "$$\n",
    "By the Bayes theorem, the posterior distribution is\n",
    "\\begin{eqnarray*}\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma})}{f(\\mathbf{y})}, \\end{eqnarray*}\n",
    "where $f$ denotes corresponding density. \n",
    "\n",
    "Note that\n",
    "\\begin{eqnarray*}\n",
    "f(\\boldsymbol{\\gamma}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\gamma}\\Bigg), \\\\ \n",
    "f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma})^T(\\sigma^2 \\mathbf{I}_n)^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma})\\Bigg) \\\\\n",
    "f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T\\mathbf{Z}\\boldsymbol{\\gamma} - \\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\Bigg) \\\\\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &\\propto& f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) f(\\boldsymbol{\\gamma}) \\\\\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\gamma} -\\frac{1}{2}\\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T\\mathbf{Z}\\boldsymbol{\\gamma} - \\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\Bigg) \\\\\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\boldsymbol{\\gamma}^T(\\boldsymbol{\\Sigma}^{-1} + \\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z}) \\boldsymbol{\\gamma}^T - \\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\Bigg)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Therefore, using properties of normal distributions and completing the square, we know that \n",
    "\n",
    "$$\\mathbf{y} \\mid \\boldsymbol{\\gamma} \\sim N(A^{-1}b, A^{-1})$$,\n",
    "\n",
    "where\n",
    "$$A = \\boldsymbol{\\Sigma}^{-1} + \\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z}, \\quad b = \\sigma^{-2}\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})$$\n",
    "\n",
    "Therefore, by the Woodbury Identity\n",
    "\n",
    "$$\\text{Var} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = (\\boldsymbol{\\Sigma}^{-1} + \\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z})^{-1}$$\n",
    "$$\\text{Var} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}\\mathbf{Z}\\boldsymbol{\\Sigma}$$,\n",
    "\n",
    "and additionally\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\sigma^{-2} (\\sigma^{-2} \\mathbf{Z}^T \\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1 } \\mathbf{Z}^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\sigma^{-2}(\\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}\\mathbf{Z}\\boldsymbol{\\Sigma})\\mathbf{Z}^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{-2}\\mathbf{I} - \\sigma^{-2}(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}(\\sigma^{-2}\\mathbf{I}(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T) - \\sigma^{-2}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}(\\mathbf{I} + \\sigma^{-2}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T - \\sigma^{-2}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Derive EM Algorithm\n",
    "\n",
    "1. Write down the complete log-likelihood\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\sum_{i=1}^m \\log f(\\mathbf{y}_i, \\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) &=& \\sum_{i=1}^m [\\log f(\\mathbf{y}_i \\mid  \\boldsymbol{\\gamma}_i,\\boldsymbol{\\beta}, \\sigma^2) + \\log f( \\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\Sigma})] \\\\\n",
    "&=& \\sum_{i=1}^m \\Bigg[- \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det (\\sigma^{2}\\mathbf{I}_{n_i}) - \\frac{1}{2} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)^T \\sigma^{-2}\\mathbf{I}_{n_i} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)\n",
    "- \\frac{q}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Sigma} - \\frac{1}{2} \\boldsymbol{\\gamma}_i^T \\Sigma^{-1}\\boldsymbol{\\gamma}_i\\Bigg] \\\\\n",
    "&=& \\sum_{i=1}^m \\Bigg[- \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det (\\sigma^{2}\\mathbf{I}_{n_i}) - \\frac{1}{2} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)^T \\sigma^{-2}\\mathbf{I}_{n_i} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)\n",
    " - \\frac{1}{2} \\boldsymbol{\\gamma}_i^T \\Sigma^{-1}\\boldsymbol{\\gamma}_i\\Bigg] - \\frac{qm}{2} \\log (2\\pi) - \\frac{m}{2} \\log \\det \\boldsymbol{\\Sigma}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "2. Derive the $Q$ function (E-step).\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Q(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2 \\mid \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^{2(t)})\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Objective of a single datum\n",
    "\n",
    "We modify the code from HW4 to evaluate the objective, the conditional mean of $\\boldsymbol{\\gamma}$, and the conditional variance of $\\boldsymbol{\\gamma}$. Start-up code is provided below. You do _not_ have to use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # posterior mean and variance of random effects γ\n",
    "    μγ         :: Vector{T} # posterior mean of random effects\n",
    "    νγ         :: Matrix{T} # posterior variance of random effects\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    rtr        :: Vector{T}\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    ztr        :: Vector{T}\n",
    "    ltztr      :: Vector{T}\n",
    "    xtr        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    ltztzl     :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    I3         :: Matrix{T}\n",
    "    Linv       :: Matrix{T}\n",
    "    storage_qq2:: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "    y::Vector{T}, \n",
    "    X::Matrix{T}, \n",
    "    Z::Matrix{T}) where T <: AbstractFloat\n",
    "    n, p, q = size(X, 1), size(X, 2), size(Z, 2)\n",
    "    μγ         = Vector{T}(undef, q)\n",
    "    νγ         = Matrix{T}(undef, q, q)\n",
    "    yty        = abs2(norm(y))\n",
    "    rtr        = Vector{T}(undef, 1)\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y\n",
    "    ztr        = similar(zty)\n",
    "    ltztr      = similar(zty)\n",
    "    xtr        = Vector{T}(undef, p)\n",
    "    storage_p  = similar(xtr)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    ltztzl     = similar(ztz)\n",
    "    storage_qq = similar(ztz)\n",
    "    I3         = Matrix{T}(I, q, q)\n",
    "    Linv       = Matrix{T}(undef, q, q)\n",
    "    storage_qq2 = Matrix{T}(undef, q, q)\n",
    "    LmmObs(y, X, Z, μγ, νγ, \n",
    "        yty, rtr, xty, zty, ztr, ltztr, xtr,\n",
    "        storage_p, storage_q, \n",
    "        xtx, ztx, ztz, ltztzl, storage_qq, \n",
    "        I3, Linv, storage_qq2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, Σ, L, σ², updater = false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `Σ`, \n",
    "and `σ²`. The lower triangular Cholesky factor `L` of `Σ` must be supplied too.\n",
    "The fields `obs.μγ` and `obs.νγ` are overwritten by the posterior mean and \n",
    "posterior variance of random effects. If `updater==true`, fields `obs.ztr`, \n",
    "`obs.xtr`, and `obs.rtr` are updated according to input parameter values. \n",
    "Otherwise, it assumes these three fields are pre-computed. \n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs     :: LmmObs{T}, \n",
    "        β       :: Vector{T}, \n",
    "        Σ       :: Matrix{T},\n",
    "        L       :: Matrix{T},\n",
    "        σ²      :: T,\n",
    "        updater :: Bool = false\n",
    "        ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    σ²inv   = inv(σ²)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################\n",
    "    # form the q-by-q matrix: Lt Zt Z L\n",
    "    copy!(obs.ltztzl, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.ltztzl) # O(q^3) obs.ltztzl = Zt Z L\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.ltztzl) # O(q^3) obs.ltztzl = Lt Zt Z L\n",
    "    # form the q-by-q matrix: M = σ² I + Lt Zt Z L\n",
    "    copy!(obs.storage_qq, obs.ltztzl)\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ² # obs.storage_qq = σ² I + Lt Zt Z L\n",
    "    end\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3) # obs.storage_qq = Rt\n",
    "    # Zt * res\n",
    "    updater && BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.ztr, obs.zty)) # O(pq)\n",
    "    # Lt * (Zt * res)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, copy!(obs.ltztr, obs.ztr))    # O(q^2)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, copy!(obs.storage_q, obs.ltztr)) # O(q^3)\n",
    "    # Xt * res = Xt * y - Xt * X * β\n",
    "    updater && BLAS.gemv!('N', T(-1), obs.xtx, β, T(1), copy!(obs.xtr, obs.xty))\n",
    "    # l2 norm of residual vector\n",
    "    updater && (obs.rtr[1] = obs.yty - dot(obs.xty, β) - dot(obs.xtr, β))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q # log det term\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (obs.rtr[1] - qf) * σ²inv \n",
    "    logl /= -2\n",
    "    ######################################\n",
    "    # TODO: Evaluate posterior mean and variance\n",
    "    ######################################    \n",
    "    \n",
    "    # Calculate Variance\n",
    "    BLAS.trsm!('R', 'L', 'N', 'N', T(1), L, copy!(obs.Linv, obs.I3)) \n",
    "    BLAS.gemm!('T', 'N', T(1), obs.Linv, obs.Linv, σ²inv, copy!(obs.storage_qq2, obs.ztz))\n",
    "    LAPACK.potrf!('L', obs.storage_qq2)\n",
    "    BLAS.trsm!('R', 'L', 'N', 'N', T(1), obs.storage_qq2, copy!(obs.νγ, obs.I3))\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), obs.νγ, obs.νγ)\n",
    "\n",
    "    # Calculate Expected Value\n",
    "    BLAS.gemv!('N', σ²inv, obs.νγ, obs.ztr, T(0), obs.μγ)\n",
    "    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/posterior mean/var evaluator here. It's the same test datum in HW2 and HW4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X = [ones(n) randn(n, p - 1)]\n",
    "Z = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, Σ, L, σ², true) = -3247.4568580638243\n",
      "obs.μγ = [-1.7352999248283547, -1.2234665777048983, -0.25020190407763465]\n",
      "obs.νγ = [0.0007495521480103862 4.188026819522356e-6 8.595028349011145e-6; 4.188026819522356e-6 0.0007599372708603274 -1.0092121486077345e-5; 8.595028349011145e-6 -1.0092121486077345e-5 0.0007370698232610101]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, Σ, L, σ², true)\n",
    "@show obs.μγ\n",
    "@show obs.νγ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3247.4568580638247)) < 1e-8\n",
    "@assert norm(obs.μγ - [-1.7352999248278138, \n",
    "        -1.2234665777052611, -0.25020190407767146]) < 1e-8\n",
    "@assert norm(obs.νγ - [0.0007495521482876466 4.188026899159083e-6 8.595028393969659e-6; \n",
    "        4.1880268803062436e-6 0.0007599372708508531 -1.0092121451703577e-5; \n",
    "        8.595028373480989e-6 -1.009212147054782e-5 0.0007370698230021235]) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficiency\n",
    "Benchmark for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.320 μs (0.00% GC)\n",
       "  median time:      1.360 μs (0.00% GC)\n",
       "  mean time:        1.497 μs (0.00% GC)\n",
       "  maximum time:     8.090 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_obj = @benchmark logl!($obs, $β, $Σ, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_obj).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
