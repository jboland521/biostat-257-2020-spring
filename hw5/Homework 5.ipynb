{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOSTAT 257: Homework 5\n",
    "### Joanna Boland\n",
    "\n",
    "Again we continue with the linear mixed effects model (LMM)\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma_0^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T.\n",
    "$$\n",
    "Given $m$ independent data points $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma_0^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma_0^2).\n",
    "$$\n",
    "\n",
    "In HW4, we used the nonlinear programming (NLP) approach (Newton type algorithms) for optimization. In this assignment, we derive and implement an expectation-maximization (EM) algorithm for the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, Distributions, LinearAlgebra, Random, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Refresher on Normal-Normal Model\n",
    "\n",
    "Assume the conditional distribution\n",
    "$$\n",
    "\\mathbf{y} \\mid \\boldsymbol{\\gamma} \\sim N(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma}, \\sigma^2 \\mathbf{I}_n)\n",
    "$$\n",
    "and the prior distribution\n",
    "$$\n",
    "\\boldsymbol{\\gamma} \\sim N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}).\n",
    "$$\n",
    "By the Bayes theorem, the posterior distribution is\n",
    "\\begin{eqnarray*}\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\frac{f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) \\times f(\\boldsymbol{\\gamma})}{f(\\mathbf{y})}, \\end{eqnarray*}\n",
    "where $f$ denotes corresponding density. \n",
    "\n",
    "Note that\n",
    "\\begin{eqnarray*}\n",
    "f(\\boldsymbol{\\gamma}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\gamma}\\Bigg), \\\\ \n",
    "f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma})^T(\\sigma^2 \\mathbf{I}_n)^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z} \\boldsymbol{\\gamma})\\Bigg) \\\\\n",
    "f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T\\mathbf{Z}\\boldsymbol{\\gamma} - \\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\Bigg) \\\\\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &\\propto& f(\\mathbf{y} \\mid \\boldsymbol{\\gamma}) f(\\boldsymbol{\\gamma}) \\\\\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\boldsymbol{\\gamma}^T\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\gamma} -\\frac{1}{2}\\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T\\mathbf{Z}\\boldsymbol{\\gamma} - \\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\Bigg) \\\\\n",
    "f(\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &\\propto& \\text{exp}\\Bigg(-\\frac{1}{2}\\boldsymbol{\\gamma}^T(\\boldsymbol{\\Sigma}^{-1} + \\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z}) \\boldsymbol{\\gamma}^T - \\sigma^{-2}\\boldsymbol{\\gamma}^T\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\Bigg)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Therefore, using properties of normal distributions and completing the square, we know that \n",
    "\n",
    "$$\\mathbf{y} \\mid \\boldsymbol{\\gamma} \\sim N(A^{-1}b, A^{-1})$$,\n",
    "\n",
    "where\n",
    "$$A = \\boldsymbol{\\Sigma}^{-1} + \\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z}, \\quad b = \\sigma^{-2}\\mathbf{Z}^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})$$\n",
    "\n",
    "Therefore, by the Woodbury Identity\n",
    "\n",
    "$$\\text{Var} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = (\\boldsymbol{\\Sigma}^{-1} + \\sigma^{-2}\\mathbf{Z}^T\\mathbf{Z})^{-1}$$\n",
    "$$\\text{Var} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) = \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}\\mathbf{Z}\\boldsymbol{\\Sigma}$$,\n",
    "\n",
    "and additionally\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E} (\\boldsymbol{\\gamma} \\mid \\mathbf{y}) &=& \\sigma^{-2} (\\sigma^{-2} \\mathbf{Z}^T \\mathbf{Z} + \\boldsymbol{\\Sigma}^{-1})^{-1 } \\mathbf{Z}^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\sigma^{-2}(\\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}\\mathbf{Z}\\boldsymbol{\\Sigma})\\mathbf{Z}^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{-2}\\mathbf{I} - \\sigma^{-2}(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}(\\sigma^{-2}\\mathbf{I}(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T) - \\sigma^{-2}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}(\\mathbf{I} + \\sigma^{-2}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T - \\sigma^{-2}\\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) \\\\\n",
    "&=& \\boldsymbol{\\Sigma}\\mathbf{Z}^T(\\sigma^{2}\\mathbf{I} + \\mathbf{Z}\\boldsymbol{\\Sigma}\\mathbf{Z}^T)^{-1}(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Derive EM Algorithm\n",
    "\n",
    "1. Write down the complete log-likelihood\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\sum_{i=1}^m \\log f(\\mathbf{y}_i, \\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) &=& \\sum_{i=1}^m [\\log f(\\mathbf{y}_i \\mid  \\boldsymbol{\\gamma}_i,\\boldsymbol{\\beta}, \\sigma^2) + \\log f( \\boldsymbol{\\gamma}_i \\mid \\boldsymbol{\\Sigma})] \\\\\n",
    "&=& \\sum_{i=1}^m \\Bigg[- \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det (\\sigma^{2}\\mathbf{I}_{n_i}) - \\frac{1}{2} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)^T \\sigma^{-2}\\mathbf{I}_{n_i} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)\n",
    "- \\frac{q}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Sigma} - \\frac{1}{2} \\boldsymbol{\\gamma}_i^T \\Sigma^{-1}\\boldsymbol{\\gamma}_i\\Bigg] \\\\\n",
    "&=& \\sum_{i=1}^m \\Bigg[- \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det (\\sigma^{2}\\mathbf{I}_{n_i}) - \\frac{1}{2} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)^T \\sigma^{-2}\\mathbf{I}_{n_i} (\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta} - \\mathbf{Z}_i \\boldsymbol{\\gamma}_i)\n",
    " - \\frac{1}{2} \\boldsymbol{\\gamma}_i^T \\Sigma^{-1}\\boldsymbol{\\gamma}_i\\Bigg] - \\frac{qm}{2} \\log (2\\pi) - \\frac{m}{2} \\log \\det \\boldsymbol{\\Sigma}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "2. Derive the $Q$ function (E-step).\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Q(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2 \\mid \\boldsymbol{\\beta}^{(t)}, \\boldsymbol{\\Sigma}^{(t)}, \\sigma^{2(t)})\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Objective of a single datum\n",
    "\n",
    "We modify the code from HW4 to evaluate the objective, the conditional mean of $\\boldsymbol{\\gamma}$, and the conditional variance of $\\boldsymbol{\\gamma}$. Start-up code is provided below. You do _not_ have to use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
